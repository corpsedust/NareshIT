{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Understanding Machine Learning Concepts**\n",
    "\n",
    "### **1. Bias-Variance Tradeoff**\n",
    "- **Bias**: Error introduced when a model makes simplistic assumptions, leading to underfitting.\n",
    "- **Variance**: Error caused by excessive complexity in the model, leading to overfitting.\n",
    "- **Bias-Variance Tradeoff**: Finding the right balance between bias and variance to create a generalizable model.\n",
    "\n",
    "### **2. Overfitting and Underfitting**\n",
    "- **Overfitting**: When a model learns noise instead of patterns, performing well on training data but poorly on unseen data.\n",
    "- **Underfitting**: When a model is too simple and fails to capture patterns in the data, leading to poor performance on both training and test data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Regression in Machine Learning**\n",
    "Regression is a supervised learning technique used to predict continuous outcomes. Some common regression algorithms include:\n",
    "- **Linear Regression**\n",
    "- **Polynomial Regression**\n",
    "- **Decision Tree Regression**\n",
    "- **Random Forest Regression**\n",
    "\n",
    "---\n",
    "\n",
    "## **Techniques to Prevent Overfitting**\n",
    "1. **Principal Component Analysis (PCA)** – Reduces dimensionality while preserving the most important variance in the data.\n",
    "2. **Cross Validation** – Splitting data into multiple training/testing sets to ensure robustness.\n",
    "3. **Regularization** – Penalizing large coefficients in regression models to prevent excessive complexity.\n",
    "   - Ridge Regularization (L2)\n",
    "   - Lasso Regularization (L1)\n",
    "   - Elastic Net (Combination of L1 & L2)\n",
    "4. **Ensemble Learning** – Combining multiple models to improve generalization.\n",
    "5. **Dropout (for Neural Networks)** – Randomly dropping neurons during training to prevent dependency on specific paths.\n",
    "6. **Business Understanding** – Understanding the real-world application of the model to avoid overfitting irrelevant patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **Feature Engineering**\n",
    "### **Feature Selection & Elimination**\n",
    "- **Backward Elimination** – Iteratively removing the least significant features.\n",
    "- **Recursive Feature Elimination (RFE)** – Training a model and recursively removing less important features.\n",
    "\n",
    "### **Feature Scaling**\n",
    "- **Min-Max Scaling (Normalization)** – Scales features to a fixed range (0 to 1).\n",
    "- **Standardization (Standard Scaler)** – Transforms data to a Gaussian distribution (mean=0, variance=1).\n",
    "\n",
    "### **Regularization in ML**\n",
    "Regularization prevents overfitting by constraining feature coefficients:\n",
    "- **Ridge Regression (L2)** – Penalizes squared magnitude of coefficients.\n",
    "- **Lasso Regression (L1)** – Shrinks some coefficients to zero, performing feature selection.\n",
    "- **Elastic Net** – A mix of Ridge and Lasso.\n",
    "\n",
    "---\n",
    "\n",
    "## **Encoding Techniques in Machine Learning**\n",
    "### **Exploratory Data Analysis (EDA) Encoders**\n",
    "- **Label Encoding** – Assigns numerical values to categorical variables.\n",
    "- **One-Hot Encoding** – Converts categorical variables into binary columns.\n",
    "- **Target Encoding** – Replaces categories with mean of target variable.\n",
    "- **Frequency Encoding** – Assigns values based on frequency of occurrence.\n",
    "- **Binary Encoding** – Converts categories into binary representations.\n",
    "\n",
    "### **ML Encoders & Multicollinearity**\n",
    "- **Multicollinearity** occurs when independent variables are highly correlated, affecting model interpretability.\n",
    "- Techniques like **Variance Inflation Factor (VIF)** help detect and remove multicollinearity.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
